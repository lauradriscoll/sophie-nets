%% YANG
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% set up data for lfads
date = '20180730';
blocks = [4:8 10:11 13:18];

LFADS_data_dir = fullfile('/home/laura/data/yangnet',date,'process/LFADS');
filename = ['LFADS_' date '_block'];

data = [];
for block = blocks
data = data_process_LFADS_join(data,LFADS_data_dir,filename,block,blocks);
end
dc.name = 'yangnet';
dataset_name = [filename '_join_' num2str(blocks(1)) '_' num2str(blocks(end)) '.mat'];
dc = YangExperiment.DatasetCollection(fullfile(LFADS_data_dir,'join'));
ds = YangExperiment.Dataset(dc, dataset_name); % adds this dataset to the collection
dc.loadInfo(); % loads dataset metadata

%% Run a single model for each dataset, and one stitched run with all datasets
runRoot = fullfile('~/runs');
rc = YangExperiment.RunCollection(runRoot, 'two_inputs', dc);
rc.version = 20180802;

%% run files will live at ~/data/yangnet/first_attempt/

% Setup hyperparameters, 4 sets with number of factors swept through 2,4,6,8
par = YangExperiment.RunParams;
par.name = 'two_inputs'; % completely optional
par.spikeBinMs = 2; % rebin the data at 2 ms
par.c_co_dim = 2; % no controller outputs --> no inputs to generator
par.c_batch_size = 150; % must be < 1/5 of the min trial count
par.c_gen_dim = 128; % number of units in generator RNN
par.c_ic_enc_dim = 128; % number of units in encoder RNN
par.c_learning_rate_stop = 1e-3; % we can stop really early for the demo
parSet = par.generateSweep('c_factors_dim', 40);
rc.addParams(parSet);

%% Setup which datasets are included in each run, here just the one
runName = dc.datasets(1).getSingleRunName(); % == 'single_dataset001'
rc.addRunSpec(LorenzExperiment.RunSpec(runName, dc, 1));

%% Generate files needed for LFADS input on disk
rc.prepareForLFADS();

%% Write a python script that will train all of the LFADS runs using a
% load-balancer against the available CPUs and GPUs
rc.writeShellScriptRunQueue('display', 0, 'virtualenv', 'py27');

% %% LORENZ
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% datasetPath = '~/lorenz_example/datasets';
% LFADS.Utils.generateDemoDatasets(datasetPath, 'nDatasets', 3);
% %%
% dataPath = '~/lorenz_example/datasets';
% dc = LorenzExperiment.DatasetCollection(dataPath);
% dc.name = 'lorenz_example';
% LorenzExperiment.Dataset(dc, 'dataset001.mat');
% LorenzExperiment.Dataset(dc, 'dataset002.mat');
% LorenzExperiment.Dataset(dc, 'dataset003.mat');
% %%
% dc.loadInfo();
% dc.getDatasetInfoTable   
% 
% %%
% runRoot = '~/lorenz_example/runs';
% rc = LorenzExperiment.RunCollection(runRoot, 'exampleSingleRun', dc);
% 
% % replace with approximate date script authored as YYYYMMDD
% % to ensure forwards compatibility
% rc.version = 20180802;
% %%
% par = LorenzExperiment.RunParams;
% par.name = 'first_attempt'; % completely optional
% par.spikeBinMs = 2; % rebin the data at 2 ms
% par.c_co_dim = 0; % no controller --> no inputs to generator
% par.c_batch_size = 150; % must be < 1/5 of the min trial count
% par.c_factors_dim = 8; % and manually set it for multisession stitched models
% par.c_gen_dim = 64; % number of units in generator RNN
% par.c_ic_enc_dim = 64; % number of units in encoder RNN
% par.c_learning_rate_stop = 1e-3; % we can stop training early for the demo
% rc.addParams(par);
% %%
% ds_index = 1;
% runSpecName = dc.datasets(ds_index).getSingleRunName(); % generates a simple run name from this datasets name
% runSpec = LorenzExperiment.RunSpec(runSpecName, dc, ds_index);
% rc.addRunSpec(runSpec);
% rc.prepareForLFADS();
% %%
% rc.writeShellScriptRunQueue('display', 0, 'virtualenv', 'py27');
% %%
% run = rc.runs('single_dataset001', 1);
% pm = run.loadPosteriorMeans();
% rates1 = squeeze(pm.rates(1, :, :)); % time x trials
% 
% 
% 
% 
% 
